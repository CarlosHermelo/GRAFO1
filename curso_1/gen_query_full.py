print("üîµ INICIANDO SCRIPT... (Cargando m√≥dulos)")

import sys
import os
import json

# --- IMPORTS ---
try:
    from typing import List, Dict, Any
    from dotenv import load_dotenv
    from neo4j import GraphDatabase
    from openai import OpenAI
    from langchain_chroma import Chroma
    from langchain_openai import OpenAIEmbeddings
    print("‚úÖ Librer√≠as importadas correctamente.")
except ImportError as e:
    print(f"‚ùå ERROR DE IMPORTACI√ìN: {e}")
    print("Ejecuta: pip install langchain-chroma langchain-openai langchain-community chromadb neo4j openai python-dotenv")
    sys.exit(1)

# --- CLASES ---

class GraphEngine:
    def __init__(self, uri, user, password, llm_model, client_openai):
        print("   ‚Ü≥ Conectando a Neo4j...")
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
        self.client_openai = client_openai
        self.llm_model = llm_model
        self.schema_summary = self._get_schema_summary()
    
    def close(self):
        self.driver.close()

    def _get_schema_summary(self) -> str:
        with self.driver.session() as session:
            try:
                session.run("RETURN 1") # Test de conexi√≥n
                sample_q = "MATCH (a)-[r]->(b) RETURN distinct labels(a)[0] as source, type(r) as rel, labels(b)[0] as target LIMIT 10"
                triplets = [f"({r['source']}) -[:{r['rel']}]-> ({r['target']})" for r in session.run(sample_q)]
                return "Patrones:\n" + "\n".join(triplets) if triplets else "Sin relaciones detectadas."
            except Exception as e:
                return f"Error leyendo esquema: {e}"

    def query(self, user_question: str) -> Dict[str, Any]:
        """
        Devuelve un diccionario con: {'cypher': str, 'data': str}
        """
        # PROMPT CORREGIDO: Instrucciones expl√≠citas sobre IDs y B√∫squeda Flexible
        system_prompt = f"""
        Eres un experto desarrollador de Neo4j.
        
        ESQUEMA DE LA BASE DE DATOS:
        {self.schema_summary}
        
        REGLAS CR√çTICAS PARA GENERAR CYPHER:
        1. **PRIORIDAD A IDs**: Los nodos tienen una propiedad `id` en formato SNAKE_CASE_MAYUSCULA (ej. "HIPERTENSION_ARTERIAL", "DIABETES_TIPO_2").
           - Tu primera estrategia debe ser convertir la entidad del usuario a este formato y buscar por `id`.
           - Ejemplo: MATCH (n:Enfermedad {{id: 'HIPERTENSION_ARTERIAL'}})
        
        2. **B√öSQUEDA POR NOMBRE**: Si no est√°s seguro del ID, busca por la propiedad `nombre` usando `CONTAINS` y `toLower()` para ser flexible.
           - NUNCA asumas coincidencia exacta en el nombre (ej. NO hagas {{nombre: 'X'}}).
           - Ejemplo: WHERE toLower(n.nombre) CONTAINS 'hipertension'
        
        3. Usa SOLO los labels y relaciones provistos en el esquema.
        4. Devuelve SOLAMENTE el c√≥digo Cypher limpio, sin markdown (```cypher).
        """
        
        try:
            # 1. Generar Cypher
            response = self.client_openai.chat.completions.create(
                model=self.llm_model,
                messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_question}],
                temperature=0
            )
            # Limpieza robusta de markdown
            cypher = response.choices[0].message.content.strip()
            cypher = cypher.replace("```cypher", "").replace("```", "").strip()
            
            # 2. Ejecutar Cypher
            with self.driver.session() as session:
                data_raw = [r.data() for r in session.run(cypher)]
            
            # Formateo del resultado
            result_str = json.dumps(data_raw, ensure_ascii=False) if data_raw else "Sin resultados directos en el Grafo."
            
            return {"cypher": cypher, "data": result_str}
            
        except Exception as e:
            return {"cypher": "ERROR", "data": f"Excepci√≥n en Grafo: {e}"}
class VectorEngine:
    def __init__(self, path_bdv, nombre_coleccion):
        print(f"   ‚Ü≥ Conectando a Chroma en {path_bdv}...")
        self.vector_store = Chroma(
            client=None,
            collection_name=nombre_coleccion,
            persist_directory=path_bdv,
            embedding_function=OpenAIEmbeddings(model="text-embedding-3-small")
        )
        
    def query(self, user_question: str) -> str:
        try:
            results = self.vector_store.similarity_search(user_question, k=3)
            if not results:
                return "Sin resultados vectoriales."
            
            # Formateamos bonito para mostrar
            formatted_res = []
            for i, doc in enumerate(results):
                formatted_res.append(f"[Chunk {i+1}]: {doc.page_content[:200]}...") # Muestra solo primeros 200 chars
            
            return "\n".join(formatted_res)
        except Exception as e:
            return f"Error Vector: {e}"

def synthesize(client, model, question, graph_data, vector_data, goal):
    system_prompt = """
    Eres un analista de informaci√≥n estricto. Tu √∫nica funci√≥n es sintetizar los datos proporcionados.
    
    REGLAS ABSOLUTAS (GUARDRAILS):
    1. üö´ PROHIBIDO usar conocimiento previo o externo. No inventes ni agregues informaci√≥n que no est√© en "DATOS PROPORCIONADOS".
    2. Si la informaci√≥n necesaria para responder NO est√° en los datos (Grafo o Vectorial), debes responder EXACTAMENTE:
       "No dispongo de informaci√≥n suficiente en la base de datos o documentos para responder a esta consulta."
    3. Cita la fuente de tu respuesta (ej. "Seg√∫n el grafo..." o "Seg√∫n el documento...").
    4. Si los datos son contradictorios, se√±√°lalo.
    """

    user_prompt = f"""
    {goal}
    
    DATOS PROPORCIONADOS (√öNICA FUENTE DE VERDAD):
    ---------------------
    [FUENTE 1: GRAFO DE CONOCIMIENTO (Estructura y Relaciones)]
    {graph_data}
    
    [FUENTE 2: B√öSQUEDA VECTORIAL (Contexto y Texto)]
    {vector_data}
    ---------------------
    
    Pregunta del usuario: "{question}"
    
    Respuesta (basada EXCLUSIVAMENTE en los datos de arriba):
    """
    
    res = client.chat.completions.create(
        model=model, 
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0 # Temperatura 0 para m√°xima determinismo y m√≠nima creatividad
    )
    return res.choices[0].message.content
# --- MAIN ---
def main():
    load_dotenv()
    print("üìÇ Variables de entorno cargadas.")

    # Validar variables
    if not all([os.getenv("NEO4J_URI"), os.getenv("BDV"), os.getenv("CARPETA_TXT")]):
        print("‚ùå Faltan variables en .env")
        return

    try:
        client_ai = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # Instanciar
        ge = GraphEngine(os.getenv("NEO4J_URI"), os.getenv("NEO4J_USER"), os.getenv("NEO4J_PASSWORD"), os.getenv("MODELO"), client_ai)
        ve = VectorEngine(os.getenv("BDV"), os.getenv("FILE_BDV"))
        
        # Cargar Goal
        goal_path = os.path.join(os.getenv("CARPETA_TXT"), "goal.config")
        goal = open(goal_path, 'r', encoding='utf-8').read() if os.path.exists(goal_path) else "Eres un asistente √∫til."
        
        print("\n" + "="*60)
        print("ü§ñ SISTEMA H√çBRIDO LISTO (Detalle Debug Activado)")
        print("="*60)
        
        while True:
            q = input("\nüó£Ô∏è Pregunta (o 'salir'): ")
            if q.lower() in ['salir', 'exit']: break
            
            print("\n" + "-"*30)
            print("üîé 1. CONSULTA AL GRAFO (NEO4J)")
            print("-" * 30)
            
            # Consulta Grafo
            g_response = ge.query(q)
            print(f"üìù [QUERY CYPHER GENERADA]:\n{g_response['cypher']}")
            print(f"\nüì¶ [RESULTADO JSON]:\n{g_response['data']}")
            
            print("\n" + "-"*30)
            print("üîé 2. CONSULTA VECTORIAL (CHROMA)")
            print("-" * 30)
            
            # Consulta Vectorial
            v_response = ve.query(q)
            print(f"üìö [CHUNKS RECUPERADOS]:\n{v_response}")
            
            print("\n" + "-"*30)
            print("üß† 3. S√çNTESIS (COMBINACI√ìN)")
            print("-" * 30)
            
            # S√≠ntesis Final
            final = synthesize(client_ai, os.getenv("MODELO"), q, g_response['data'], v_response, goal)
            print(f"üí° RESPUESTA FINAL:\n{final}")
            
            print("\n" + "="*60)
            
        ge.close()
        
    except Exception as e:
        print(f"‚ùå ERROR CRITICO EN MAIN: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()